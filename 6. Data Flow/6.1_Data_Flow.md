# Running the Spark Application using Dataflow
This involves executing a distributed **Apache Spark job** on **OCI** using the Data Flow service. This step allowed me to process large-scale data (e.g., from **Object Storage**) without managing infrastructure. 

## Settings
Before creating a Data Flow we must complete the following:

### Dynamic Group Creation
I navigated to *Menu* (☰) > *Identity & Security* > *Domains*. There, I selected the domain I was using for this project, in my case, the **Default** domain. 

Once inside the domain, I clicked on *Dynamic groups* from the ribbon options. Then, *Create dynamic group*. I provided the following information:
- **Name**: dataflow_dynamic_group
- **Description**: Dynamic group for Data Flow access to Object Storage
- **Matching rules**: Match all rules defined below
- **Rule 1**: ALL {resource.type = 'dataflowrun'}

And clicked *Create*.

### Dynamic Group Policy Creation
After that, I navigated to *Menu* (☰) > *Identity & Security* > *Policies* > *Create Policy*.
- **Name**: dataflow_dynamic_group_policy
- **Description**: Allow dynamic-group dataflow_dynamic_group to read object-family in tenancy
- **Compartment**: root (in my case, as I’m working inside it)
- **Policy Builder** (manual editor): Allow dynamic-group YOUR DYNAMIC GROUP NAME (in this case **dataflow_dynamic_group**) to read object-family in compartment NAME OF THE COMPARTMENT YOU ARE USING

## Application Creation in Data Flow
Inside OCI, I navigated to *Menu* (☰) > *Analytics and AI* > *Data flow* > *Applications*. I made sure the selected compartment was **Root**, as that is the one I am using, and clicked *Create Application*. I provided the following information:
- **Name**: Datalake_dataflow
- **Language**: SQL
- **Select a file**: ensure the compartment your using is selected and, for bucket, select **datacontent**, while for file, select the **sql_report_datalake.sql** document.
- **Parameters Name**: location
- **Parameters Value**: oci://YOUR BUCKET NAME @ YOUR NAMESPACE NAME/


And clicked *Create*.

## Running the Application
I clicked the *Options Symbol* (⋮) from the newly created application **Datalake_dataflow** > *Run*. 

A window popped up for me to customize parameters, and I set the following:

- **Driver shape**: VM.Standard2.1 (15 GB, 1 OCPU)
- **Executor shape**: VM.Standard2.1 (15 GB, 1 OCPU)
- **Number of executors**: 1

And clicked *Run*.

> ### **Disclaimer**
> In my case, when trying to do this, I got a message indicating that the resources I had requested for this run exceeded my tenancy Data Flow quota limit.
>
> ![noquota](/Assets/0quota.png)
>
> Therefore, to see my exact usage limit I navigated to *Menu* (☰) inside OCI > *Governance & Administration* > under *Tenancy Management* > *Limits, Quotas, and Usage*. By selecting the service **Data Flow** I could confirm I had no available services left to use since I was using the **OCI Free Tier Account**.
>
> ![limits](/Assets/Limits.png)

## Opening Spark
Once the run has been finalized, open the run > go to run logs > and open the **spark_application STD log**. This will display the output of the Spark SQL script.

Back in the run page click *Spark UI* (useful for debugging and performance tuning). This will pop up a new window. Click *DAG Visualization* to see the various stages of execution.

The integration between **Object Storage** and **Data Flow** services is now successfully completed.